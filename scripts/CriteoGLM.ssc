import org.apache.spark.rdd.RDD
import scala.reflect.ClassTag
import BIDMat.{FMat,DMat}
import BIDMat.MatIO
import BIDMat.MatFunctions._
import BIDMat.SciFunctions._
import BIDMat.Solvers._
import BIDMach.RunOnSpark._
import BIDMach.RunOnSparkGLM._
import BIDMach.Learner
import BIDMach.models.{Model, GLM}
import org.apache.hadoop.io.Text
import org.apache.spark.HashPartitioner
import org.apache.spark.storage.StorageLevel

// Specify IP address of master here
val MASTER_DNS = java.net.InetAddress.getLocalHost.getHostAddress
val numExecutors = 4

val rddData = sc.sequenceFile(
   "hdfs://%s:9000/criteo/criteo_merged_82.lz4".format(MASTER_DNS),
   classOf[Text],
   classOf[BIDMat.MatIO]
).partitionBy(new HashPartitioner(numExecutors)).persist(StorageLevel.MEMORY_AND_DISK)

val (learner, opts) = GLM.learner()
opts.useGPU = true
opts.batchSize = 10000
opts.lrate = 0.05f
opts.texp = 0.5f
opts.reg1weight = 0.0f
opts.npasses = 5
opts.useGPU = true
opts.links = irow(1)
opts.aopts = opts

var models:Array[Model] = null
def time[R](block: => R):R = {
    val t0 = System.nanoTime()
    val result = block
    val t1 = System.nanoTime()
    println("Elapsed time: " + (t1 - t0)/math.pow(10, 9)+ "s")
    result
}
val (master, models) = time {runOnSparkGLM(sc, learner, rddData, numExecutors)}

if (models != null) {
   val matModels = models.map(_.modelmats(0))
   val mdir = "/mnt/BIDMach/data/criteo/parts/"

   val (nn,popts) = GLM.learner(mdir+"trainsorted%02d.smat.lz4", mdir+"trainlabel%02d.fmat.lz4")
   popts.useGPU = opts.useGPU
   popts.batchSize = opts.batchSize
   popts.lrate = opts.lrate
   popts.texp = opts.texp
   popts.reg1weight = opts.reg1weight
   popts.npasses = opts.npasses
   popts.useGPU = opts.useGPU
   popts.links = opts.links
   popts.aopts = opts.aopts
   nn.init

   var meanrrs = new Array[Double](numExecutors + 1)
   var llns = new Array[Double](numExecutors + 1)

   for (wi <- 0 until (numExecutors + 1)) {
     if (wi >= numExecutors) {
       nn.modelmats(0) = matModels.reduceRight(_ + _) / numExecutors
     } else {
       nn.modelmats(0) = matModels(wi)
     }

     val model = nn.model.asInstanceOf[GLM]
     val mm = FMat(model.modelmats(0))

     val ntest = 10
     val filesize = 500000
     val tscores = dzeros(filesize*ntest, 1)
     val tcats = dzeros(filesize*ntest, 1)
     var len = 0
     for (i <- 0 until ntest) {
       val a = loadSMat(mdir + "trainsorted%02d.smat.lz4" format i + 80)
       val c = loadFMat(mdir + "trainlabel%02d.fmat.lz4" format i + 80)
       val sc = mm * a
       tscores(i * filesize -> (i * filesize + sc.length), 0) = DMat(sc.t)
       tcats(i * filesize -> (i * filesize + sc.length), 0) = DMat(c.t)
       len += c.length
       print(".")
     }
     val scores = tscores(0->len,0)
     val cats = tcats(0->len,0)

     val rr = roc(scores, cats, 1-cats, 1000)

     val bounds = 7.0
     val bscores = min(max(scores, drow(-bounds)), drow(bounds))
     val probs = 1/(1 + exp(-bscores))
     val lln = ((cats dot ln(probs)) + ((1-cats) dot ln(1-probs)))/probs.length.toDouble

     meanrrs(wi) = mean(rr).v
     llns(wi) = lln.v
     var label = if (wi >= numExecutors) "avg" else ("%d" format wi)
     println("worker %s: roc area = %5.4f, ll = %5.4f" format (label, meanrrs(wi), llns(wi)))
   }
}
